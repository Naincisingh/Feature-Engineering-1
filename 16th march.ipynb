{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d47956c1-a12e-4ffe-bf07-b8d1f3f6b5ad",
   "metadata": {},
   "source": [
    "QTS.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d538f7-ca51-475a-a2a7-2e3ac05f4bc6",
   "metadata": {},
   "source": [
    "Overfitting occurs when a machine learning model learns the training data too well, \n",
    "capturing noise or random fluctuations that don't represent the true \n",
    "underlying patterns. This can lead to poor generalization on new, unseen data.\n",
    "\n",
    "Underfitting happens when a model is too simple to capture the underlying \n",
    "patterns of the training data, resulting in poor performance on both the training and new data.\n",
    "\n",
    "Consequences:\n",
    "- Overfitting: High performance on training data but poor generalization.\n",
    "- Underfitting: Poor performance on both training and new data.\n",
    "\n",
    "Mitigation:\n",
    "- Overfitting: Use techniques like regularization, cross-validation, and reduce model complexity.\n",
    "- Underfitting: Increase model complexity, add relevant features, or use more advanced models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c45560e-d08a-4ce3-8558-121a0e658569",
   "metadata": {},
   "source": [
    "QTS.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c71a63b-ab7f-4eda-8d17-04ee4bbe851d",
   "metadata": {},
   "source": [
    "To reduce overfitting:\n",
    "1. **Regularization**: Add penalty terms to the model's objective function to\n",
    "discourage overly complex models.\n",
    "2. **Cross-validation**: Evaluate the model on different subsets of the \n",
    "training data to assess generalization performance.\n",
    "3. **Data Augmentation**: Increase the diversity of the training data by\n",
    "applying transformations like rotation, scaling, or cropping.\n",
    "4. **Feature Selection**: Choose relevant features and discard irrelevant ones to reduce noise.\n",
    "5. **Ensemble Methods**: Combine predictions from multiple models to improve generalization.\n",
    "6. **Early Stopping**: Halt the training process when the performance on a validation set starts to degrade.\n",
    "7. **Pruning**: Trim unnecessary branches from decision trees to prevent them from becoming too complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896cce49-b357-4aad-bd93-f396ac052531",
   "metadata": {},
   "source": [
    "QTS.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59587455-1f05-4a82-b962-d45b0d871f9b",
   "metadata": {},
   "source": [
    "Underfitting occurs when a machine learning model is too simple to capture the \n",
    "underlying patterns in the training data,\n",
    "leading to poor performance on both the training and new data.\n",
    "\n",
    "Scenarios where underfitting can occur:\n",
    "1. **Insufficient Model Complexity**: When using a model that is too basic to \n",
    "represent the underlying relationships in the data.\n",
    "2. **Limited Training Data**: When the amount of available training data is \n",
    "insufficient for the model to learn the underlying patterns.\n",
    "3. **Inadequate Features**: When essential features are missing or not properly selected for the model.\n",
    "4. **Excessive Regularization**: When applying too much regularization, which can \n",
    "overly constrain the model's learning capacity.\n",
    "5. **Ignoring Non-linear Relationships**: When using a linear model for data with \n",
    "complex non-linear patterns.\n",
    "6. **Improper Hyperparameter Tuning**: When hyperparameters are not tuned \n",
    "appropriately for the specific problem, leading to a suboptimal model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f68c23-8da2-48c5-8b22-6cd2c080f30f",
   "metadata": {},
   "source": [
    "QTS.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7d4718d-7383-49a0-9c78-c54febfcb37d",
   "metadata": {},
   "source": [
    "The bias-variance tradeoff is a fundamental concept in machine learning. \n",
    "It represents the balance between bias (error from overly simplistic assumptions)\n",
    "and variance (error from too much complexity) in a model.\n",
    "\n",
    "- **Bias**: High bias implies the model is too simple and may overlook underlying patterns (underfitting).\n",
    "- **Variance**: High variance suggests the model is too complex and may \n",
    "capture noise in the training data (overfitting).\n",
    "\n",
    "**Relationship:**\n",
    "- As model complexity increases, bias decreases but variance increases, and vice versa.\n",
    "- There's a tradeoff because reducing one often leads to an increase in the other.\n",
    "\n",
    "**Impact on Performance:**\n",
    "- **High Bias**: Poor performance on both training and test data (underfitting).\n",
    "- **High Variance**: Good performance on training but poor on test data (overfitting).\n",
    "\n",
    "**Optimal Model:**\n",
    "- Balancing bias and variance leads to the optimal model that generalizes well to new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666b4914-fa71-4551-80a4-e7d0e5168e70",
   "metadata": {},
   "source": [
    "QTS.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e476ca-842a-48a5-9fef-c5bc81bf9888",
   "metadata": {},
   "source": [
    "Common methods for detecting overfitting and underfitting:\n",
    "\n",
    "1. **Learning Curves**: Plot the training and validation performance over time.\n",
    "Overfitting may show a large gap between the two, while underfitting may show poor performance overall.\n",
    "\n",
    "2. **Validation Metrics**: Monitor performance metrics on a validation set.\n",
    "Sudden drops in performance on the validation set may indicate overfitting.\n",
    "\n",
    "3. **Cross-Validation**: Evaluate the model on different subsets of the training \n",
    "data to assess its generalization performance. Large variability between folds may indicate overfitting.\n",
    "\n",
    "4. **Model Complexity Graphs**: Plot the model's performance against its complexity.\n",
    "Overfitting often occurs when the model becomes too complex.\n",
    "\n",
    "5. **Residual Analysis**: For regression problems, analyze the residuals \n",
    "(the differences between predicted and actual values). Patterns in residuals \n",
    "may indicate underfitting or overfitting.\n",
    "\n",
    "6. **Regularization Parameter Tuning**: Systematically vary the regularization \n",
    "strength and observe the impact on performance. A too-strong regularization may lead to underfitting.\n",
    "\n",
    "7. **Prediction Confidence**: For classification problems, examine prediction \n",
    "confidence scores. Overfit models may assign high confidence to incorrect predictions.\n",
    "\n",
    "Determining whether a model is overfitting or underfitting involves closely monitoring\n",
    "its performance on both the training and validation sets, analyzing learning curves, \n",
    "and using diagnostic tools specific to the problem type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45e73a0-781d-4755-8ba9-43742f050b59",
   "metadata": {},
   "source": [
    "QTS.6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc79c2e-7f9d-4dce-80b9-c3156d261437",
   "metadata": {},
   "source": [
    "**Bias:**\n",
    "- **Definition**: Error from overly simplistic assumptions in the model.\n",
    "- **Effect**: High bias leads to underfitting.\n",
    "- **Performance**: Poor on both training and test data.\n",
    "- **Example**: Linear regression on a non-linear dataset.\n",
    "\n",
    "**Variance:**\n",
    "- **Definition**: Error from too much complexity in the model.\n",
    "- **Effect**: High variance leads to overfitting.\n",
    "- **Performance**: Good on training, poor on test data.\n",
    "- **Example**: High-degree polynomial regression on a small dataset.\n",
    "\n",
    "**Comparison:**\n",
    "- **Bias**: Insufficiently captures underlying patterns.\n",
    "- **Variance**: Captures noise and fluctuations in the training data.\n",
    "- **Optimal Model**: Balances bias and variance for good generalization.\n",
    "- **Tradeoff**: Adjusting one often increases the other (bias-variance tradeoff).\n",
    "\n",
    "**Summary:**\n",
    "- High bias models are too simple and perform poorly on both training and test data.\n",
    "- High variance models are too complex and perform well on training data but poorly on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2089109-1828-47cc-b9e8-bd389c243b6c",
   "metadata": {},
   "source": [
    "QTS.7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d014e2-be4c-4d3a-9c67-d33eef7becc5",
   "metadata": {},
   "source": [
    "**Regularization in Machine Learning:**\n",
    "- **Definition**: Regularization is a technique used to prevent overfitting by adding a penalty term\n",
    "to the model's objective function. This penalty discourages overly complex models.\n",
    "\n",
    "**Common Regularization Techniques:**\n",
    "\n",
    "1. **L1 Regularization (Lasso):**\n",
    "   - **How it works**: Adds the absolute values of the coefficients to the loss function.\n",
    "   - **Effect**: Encourages sparsity, leading to some coefficients being exactly zero.\n",
    "\n",
    "2. **L2 Regularization (Ridge):**\n",
    "   - **How it works**: Adds the squared values of the coefficients to the loss function.\n",
    "   - **Effect**: Discourages large coefficients, preventing overemphasis on particular features.\n",
    "\n",
    "3. **Elastic Net Regularization:**\n",
    "   - **How it works**: Combines both L1 and L2 regularization.\n",
    "   - **Effect**: Balances the sparsity-inducing property of L1 with the grouping effect of L2.\n",
    "\n",
    "4. **Dropout (Neural Networks):**\n",
    "   - **How it works**: Randomly deactivates a fraction of neurons during each training iteration.\n",
    "   - **Effect**: Prevents reliance on specific neurons, improving generalization.\n",
    "\n",
    "5. **Early Stopping:**\n",
    "   - **How it works**: Halts the training process when the model's performance on a \n",
    "    validation set starts to degrade.\n",
    "   - **Effect**: Prevents the model from becoming too specialized to the training data.\n",
    "\n",
    "Regularization methods introduce constraints on the model's parameters, promoting \n",
    "simplicity and preventing overfitting to noisy training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5883d6-1410-4d56-b554-f5f381764f92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
